#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Unified-Grid SED pipeline (adaptive ~30k; all float64; HDF5 4GB-chunk safe)

- Grid: ADAPT_WAVE (~30k) shared by Stellar / AGN / Nebular / Final
- Stage A (stellar_adapt.h5): Stellar SED (ADAPT), Q_H(f64)
- Stage B (agn_adapt.h5)    : AGN SED (ADAPT)
- Stage C (neb_adapt.h5)    : Nebular SED (ADAPT) using (Q_H, Z_gas) and Te(Z_gas)
- Stage D (final.h5)        : Merge (no re-interp), attenuation on (stellar+nebular+agn),
                              dust (Casey12, T=25K, energy-balance to absorbed (stellar+neb+agn)),
                              sum (7 datasets)
- Cleanup                   : Remove temp files

All datasets are float64.
"""

# ===== Environment (pin cores / avoid oversubscription) =====
import os
os.environ.setdefault("OMP_NUM_THREADS", "1")
os.environ.setdefault("OPENBLAS_NUM_THREADS", "1")
os.environ.setdefault("MKL_NUM_THREADS", "1")
os.environ.setdefault("VECLIB_MAXIMUM_THREADS", "1")
os.environ.setdefault("NUMEXPR_NUM_THREADS", "1")

import atexit
import glob
import math
import re
from concurrent.futures import FIRST_COMPLETED, ThreadPoolExecutor, wait

import h5py
import numpy as np
from astropy.cosmology import FlatLambdaCDM
from tqdm import tqdm

# =========================
# 0) Paths / Parameters
# =========================
SNAP = "/data2/fukushima/L200N1024/snapshot_020.hdf5"
FOF  = "/data2/fukushima/L200N1024/fof_subhalo_tab_020.hdf5"

Stellar_DIR   = "/data/grqtm/Research/Keita/MIST_cube"
AGN_DIR       = "/data/grqtm/Research/Keita/Template/SKIRTOR/SKIRTOR_data"
Nebular_DIR   = "/data/grqtm/Research/Keita/Template/Nebular/forMawatari_17Jul"

NEB_LINE_FILE = os.path.join(Nebular_DIR, "LineRatio_nodust_May2016-2.txt")
NEB_CONT_FILE = os.path.join(Nebular_DIR, "rspec_cont_fe00.txt")

OUT_FINAL = "SEDs.hdf5"
TMP_STELLAR = "stellar_adapt_f64.h5"
TMP_AGN     = "agn_adapt_f64.h5"
TMP_NEB     = "nebular_adapt_f64.h5"

# =========================
# Save options (final datasets)
# =========================
# Toggle which final datasets to save.
SAVE_SETS = {
    "stellar_sed": True,
    "nebular_sed": True,
    "stellar_plus_nebular": True,
    "stellar_nebular_agn_attenuated": True,
    "dust_emission_sed": True,
    "agn_sed": True,
    "total_sed": True,
}
def SAVE(name: str) -> bool:
    return bool(SAVE_SETS.get(name, False))

# =========================
# Selection / Execution
# =========================
MSTAR_THRESH   = 1.0e9       # Msun
_CPU_COUNT = max(1, int(os.cpu_count() or 1))

# User tuning (edit these values directly for your server profile)
USER_THREADS = 20
USER_TARGET_RAM_GB = 150.0
USER_SAFETY_RAM_GB = 16.0
USER_BLOCK_ROWS_STAGE_A = 24
USER_BLOCK_ROWS_STAGE_B = 96
USER_BLOCK_ROWS_STAGE_C = 24
USER_MAX_INFLIGHT_BLOCKS = max(2, USER_THREADS * 3)

# Optional env overrides keep the same behavior for batch runs.
N_THREADS = int(os.environ.get("SED_THREADS", str(USER_THREADS)))
N_THREADS = max(1, min(N_THREADS, _CPU_COUNT))
TARGET_RAM_GB = float(os.environ.get("SED_TARGET_RAM_GB", str(USER_TARGET_RAM_GB)))
SAFETY_RAM_GB = float(os.environ.get("SED_SAFETY_RAM_GB", str(USER_SAFETY_RAM_GB)))

# Row-block settings for bounded parallel scheduling.
BLOCK_ROWS_STAGE_A = max(1, int(os.environ.get("SED_BLOCK_ROWS_STAGE_A", str(USER_BLOCK_ROWS_STAGE_A))))
BLOCK_ROWS_STAGE_B = max(1, int(os.environ.get("SED_BLOCK_ROWS_STAGE_B", str(USER_BLOCK_ROWS_STAGE_B))))
BLOCK_ROWS_STAGE_C = max(1, int(os.environ.get("SED_BLOCK_ROWS_STAGE_C", str(USER_BLOCK_ROWS_STAGE_C))))
MAX_INFLIGHT_BLOCKS = max(2, int(os.environ.get("SED_MAX_INFLIGHT_BLOCKS", str(USER_MAX_INFLIGHT_BLOCKS))))

# Cosmology / constants / units
cosmo  = FlatLambdaCDM(H0=67.742, Om0=0.3099)
Z_sun  = 0.0134
h_sim  = 0.67742
MASS_CONV = 1e10 / h_sim     # catalogue mass × MASS_CONV = Msun
A2CM   = 1e-8
c_cms  = 2.99792458e10
c_kms  = 2.99792458e5
h_ergs = 6.62607015e-27
kB     = 1.380649e-16
Lsun   = 3.839e33

ADAPT_WAVE = None

# Calzetti (2000) + Hayes (2011) E(B-V)(z)
RV_PRIME = 4.05

# Hayes+2011: E(B-V)(z) = C_EBV * exp(-z / Z_EBV)
C_EBV_HAYES = 0.386
Z_EBV_HAYES = 3.42
def ebv_from_redshift(z, C_EBV=C_EBV_HAYES, z_EBV=Z_EBV_HAYES):
    """Return mean E(B-V) as a function of redshift z (Hayes+2011)."""
    z = float(z)
    return float(C_EBV * math.exp(-z / z_EBV))

# Casey (2012) (exact in ν-space, convert to S_lambda)
T_dust = 25.0
CASEY_BETA       = 1.60
CASEY_ALPHA      = 2.0
CASEY_LAMBDA0_UM = 200.0
# Dust energy balance: absorption integration range / dust emission range
L_ABS_MIN_UM, L_ABS_MAX_UM = 0.0912, 10.0
IR_MIN_UM,  IR_MAX_UM      = 3.0, 1000.0

# Nebular line width.
FWHM_KMS = 300.0

# Metallicity grid for nebular templates (l₩ine_raw / cont_raw).
# The final element is a small placeholder representing the dedicated Z=0 template bin.
Z_FLOOR_NEB = 1e-7
Z_GRID_FOR_TPL = np.array(
    [5e-2, 2e-2, 8e-3, 4e-3, 4e-4, 1e-4, 1e-5, 1e-7, 1e-9],
    dtype=np.float64
)
IDX_Z_ZERO_TPL = int(Z_GRID_FOR_TPL.size - 1)

# Te(Z) lookup table.
# For Z < 1e-7 (or non-finite / non-positive), use the Z=0 row value directly.
Z_FLOOR_TE = 1e-7
LOGTE_ZERO = 4.310
_Z_TE_POS = np.array([5e-2, 2e-2, 8e-3, 4e-3, 4e-4, 1e-4, 1e-5, 1e-7], dtype=np.float64)
_LOGTE_POS = np.array([3.579, 3.891, 4.059, 4.147, 4.264, 4.290, 4.304, 4.308], dtype=np.float64)

# Pre-sort for interpolation
__logZ_tab = np.log10(_Z_TE_POS)
__sort = np.argsort(__logZ_tab)
__logZ_tab = __logZ_tab[__sort]
__LOGTE_tab = _LOGTE_POS[__sort]

def Te_from_Zgas(Zgas: float) -> float:
    """
    Estimate electron temperature Te [K] from gas metallicity Z (mass fraction)
    using the requested piecewise rule:
      - Z < 1e-7 (or non-finite / non-positive): fixed Z=0 row value
      - Z >= 1e-7: logZ-linear interpolation on positive-Z nodes
    """
    z = float(Zgas)
    if (not np.isfinite(z)) or (z <= 0.0) or (z < Z_FLOOR_TE):
        return 10.0**float(LOGTE_ZERO)

    z = min(max(z, float(_Z_TE_POS.min())), float(_Z_TE_POS.max()))
    logZ = math.log10(z)
    logTe = float(np.interp(logZ, __logZ_tab, __LOGTE_tab))
    return 10.0**logTe

TAIL_N = 12  # high-λ tail points for extrapolation

# Line-grid spec
LINE_W_SIGMA     = 5.0   # ±W·σ window
LINE_P_PER_FWHM  = 8     # samples per FWHM

# Registered temp files (auto-delete on exit)
TO_DELETE = set()
def register_tmp(fp):
    TO_DELETE.add(fp)

def cleanup_tmp():
    for fp in list(TO_DELETE):
        try:
            if os.path.exists(fp):
                os.remove(fp)
        except Exception:
            pass
atexit.register(cleanup_tmp)

# =========================
# 1) Memory / Interp / HDF5 utils
# =========================
def bytes_per_array(n_wave, itemsize=8):
    return int(n_wave) * int(itemsize)

def choose_rows_chunk(target_ram_gb, n_wave, arrays_in_block, itemsize=8,
                      safety_gb=8.0, hard_cap=20000, hard_min=64):
    """Heuristic: rows * n_wave * itemsize * arrays_in_block <= (target_ram_gb - safety_gb) [GB]."""
    avail = max(target_ram_gb - safety_gb, 1.0) * (1024**3)
    per_row = bytes_per_array(n_wave, itemsize=itemsize) * arrays_in_block
    rows = int(avail // max(per_row, 1))
    rows = max(hard_min, min(hard_cap, rows))
    return rows

# HDF5 4GB chunk limit
MAX_HDF5_CHUNK_BYTES = 4_000_000_000  # strictly "< 4GB"

def cap_rows_chunk_for_hdf5(n_wave, rows_chunk, dtype=np.float64,
                            max_bytes=MAX_HDF5_CHUNK_BYTES):
    """Cap rows_chunk so that chunk_size < 4 GB for f64 arrays."""
    item = np.dtype(dtype).itemsize
    max_rows = max(1, (max_bytes - 1) // (n_wave * item))
    return int(min(rows_chunk, max_rows))

def iter_row_blocks(n_rows, block_rows):
    """Yield contiguous [start, end) row blocks."""
    n_rows = int(n_rows)
    block_rows = max(1, int(block_rows))
    for start in range(0, n_rows, block_rows):
        end = min(n_rows, start + block_rows)
        yield start, end

def run_parallel_row_blocks(
    n_rows,
    block_rows,
    desc,
    unit,
    worker_fn,
    consume_fn,
    max_workers=N_THREADS,
    max_inflight_blocks=MAX_INFLIGHT_BLOCKS,
):
    """
    Run row blocks with bounded in-flight futures.

    worker_fn(start, end) -> result with row range metadata
    consume_fn(result) writes/consumes worker output
    """
    n_rows = int(n_rows)
    if n_rows <= 0:
        return

    max_workers = max(1, int(max_workers))
    max_inflight_blocks = max(1, int(max_inflight_blocks))
    blocks_iter = iter(iter_row_blocks(n_rows, block_rows))

    with ThreadPoolExecutor(max_workers=max_workers) as ex:
        pending = set()

        for _ in range(max_inflight_blocks):
            try:
                s, e = next(blocks_iter)
            except StopIteration:
                break
            pending.add(ex.submit(worker_fn, s, e))

        with tqdm(total=n_rows, desc=desc, unit=unit) as pbar:
            while pending:
                done, pending = wait(pending, return_when=FIRST_COMPLETED)
                for fu in done:
                    result = fu.result()
                    consume_fn(result)

                    start, end = result[0], result[1]
                    pbar.update(int(end) - int(start))

                    try:
                        s, e = next(blocks_iter)
                    except StopIteration:
                        continue
                    pending.add(ex.submit(worker_fn, s, e))

def ensure_sorted(x, y):
    """Ensure x is increasing; y is reordered accordingly."""
    if x[0] > x[-1]:
        idx = np.argsort(x)
        return x[idx], y[idx]
    return x, y

def precompute_interp_map_loglog(src_wave, dst_wave):
    """Precompute log-log linear interpolation indices and weights."""
    src = np.asarray(src_wave, np.float64)
    dst = np.asarray(dst_wave, np.float64)
    assert np.all(np.diff(src) > 0), "src_wave must be increasing."

    log_src = np.log(src)
    log_dst = np.log(dst)

    pos = np.searchsorted(log_src, log_dst, side="left")
    hi = np.clip(pos, 1, len(src) - 1).astype(np.int32)
    lo = (hi - 1).astype(np.int32)

    x0 = log_src[lo]
    x1 = log_src[hi]
    t = (log_dst - x0) / (x1 - x0)
    w_hi = np.clip(t, 0.0, 1.0).astype(np.float64)
    w_lo = (1.0 - w_hi).astype(np.float64)

    left_mask  = log_dst < log_src[0]
    right_mask = log_dst > log_src[-1]

    w_lo[left_mask] = w_hi[left_mask] = 0.0
    w_lo[right_mask] = w_hi[right_mask] = 0.0

    lo[left_mask] = 0
    hi[left_mask] = 0
    lo[right_mask] = len(src) - 1
    hi[right_mask] = len(src) - 1

    return lo, hi, w_lo, w_hi, left_mask, right_mask, log_src[-1]

def interp_loglog_to_grid(w_src, y_src, w_dst):
    """1D log-log interpolation (out-of-range→0, non-positive→0)."""
    w_src = np.asarray(w_src, np.float64)
    y_src = np.asarray(y_src, np.float64)
    w_dst = np.asarray(w_dst, np.float64)

    w_src, y_src = ensure_sorted(w_src, y_src)
    pos = (w_src > 0) & (y_src > 0) & np.isfinite(y_src)
    if np.count_nonzero(pos) < 2:
        return np.zeros_like(w_dst, dtype=np.float64)

    xp = np.log10(w_src[pos])
    fp = np.log10(y_src[pos])
    x  = np.log10(w_dst)

    logi = np.interp(x, xp, fp, left=-np.inf, right=-np.inf)
    out  = np.where(np.isneginf(logi), 0.0, 10.0**logi)

    out[~np.isfinite(out)] = 0.0
    out[out < 0] = 0.0
    return out.astype(np.float64, copy=False)

# =========================
# 1.5) Build adaptive wavelength grid
# =========================
def build_adaptive_wave(base_min_A, base_max_A, n_base, lam_lines,
                        fwhm_kms, W_sigma=5.0, p_per_fwhm=8,
                        edge_points=True):
    """
    Build adaptive wavelength grid:
      - base log-grid (n_base)
      - refined windows around each line center λ_j (±W·σ, ~p samples per FWHM)
      - optional refinement around 912, 3646 Å
    """
    base = np.geomspace(base_min_A, base_max_A, n_base).astype(np.float64)

    lam_lines = np.asarray(lam_lines, np.float64)
    sigma_factor = fwhm_kms / (c_kms * 2.0 * np.sqrt(2.0 * np.log(2.0)))  # σ = λ * factor
    add_pts = []

    W = float(W_sigma)
    p = int(p_per_fwhm)

    for lam in lam_lines:
        sigma = lam * sigma_factor
        fwhm  = 2.355 * sigma
        if not np.isfinite(fwhm) or fwhm <= 0:
            continue

        half_width = W * sigma
        a = max(base_min_A, lam - half_width)
        b = min(base_max_A, lam + half_width)
        if b <= a:
            continue

        n_est = int(np.ceil((2.0 * W / 2.355) * p)) + 1
        n_est = max(9, min(200, n_est))
        add_pts.append(np.linspace(a, b, n_est, dtype=np.float64))

    if edge_points:
        for edge in (912.0, 3646.0):
            a = edge * 0.995
            b = edge * 1.005
            if (b > base_min_A) and (a < base_max_A):
                add_pts.append(
                    np.geomspace(max(base_min_A, a),
                                 min(base_max_A, b),
                                 61).astype(np.float64)
                )

    if add_pts:
        extra  = np.unique(np.concatenate(add_pts))
        merged = np.unique(np.concatenate([base, extra]))
    else:
        merged = base

    return merged.astype(np.float64)

# =========================
# 2) Physics utils (float64)
# =========================
def cosmic_time_from_a(a):
    """Cosmic time [Gyr] from scale factor a."""
    return cosmo.age((1.0 / np.asarray(a)) - 1.0).value

def integrate_trapz(x, y):
    return np.trapz(np.asarray(y, np.float64),
                    np.asarray(x, np.float64))

def compute_QH(wave_A, Llam):
    """Ionizing photon production rate Q_H from L_lambda."""
    wave_A, Llam = ensure_sorted(np.asarray(wave_A, np.float64),
                                 np.asarray(Llam, np.float64))
    m = (wave_A <= 912.0)
    if not np.any(m):
        return 0.0

    lam_cm = wave_A[m] * A2CM
    L_cm   = Llam[m]   * 1e8  # per Å → per cm

    integrand = (lam_cm * L_cm) / (h_ergs * c_cms)
    return float(integrate_trapz(lam_cm, integrand))

def alpha_B(Te):
    """Case B recombination coefficient [cm^3 s^-1]."""
    T4 = Te / 1e4
    return 4.309e-13 * (T4**(-0.6166)) / (1.0 + 0.6703 * (T4**0.53))

def gamma_Hbeta(Te):
    """Volume emissivity of Hβ [erg cm^3 s^-1]."""
    return 1.23e-25 * (Te / 1e4)**(-0.9)

# =========================
# 3) Casey & Calzetti (float64)
# =========================
def k_calzetti_lambda_um(lam_um, Rv_prime=4.05):
    lam = np.asarray(lam_um, np.float64)
    lam = np.clip(lam, 1.0e-6, None)

    k = np.zeros_like(lam)

    m1 = (lam >= 0.12) & (lam < 0.63)
    m2 = (lam >= 0.63) & (lam <= 2.2)
    m_low  = lam < 0.12
    m_high = lam > 2.2

    mask_uv = m1 | m_low
    if np.any(mask_uv):
        inv = 1.0 / lam[mask_uv]
        k[mask_uv] = (
            2.659 * (-2.156 + 1.509 * inv - 0.198 * inv**2 + 0.011 * inv**3)
            + Rv_prime
        )

    mask_opt = m2 | m_high
    if np.any(mask_opt):
        inv = 1.0 / lam[mask_opt]
        k[mask_opt] = (
            2.659 * (-1.857 + 1.040 * inv)
            + Rv_prime
        )

    return k

def inv_expm1_stable(x):
    """Compute 1/(e^x - 1) stably for wide x range."""
    x = np.asarray(x, np.float64)
    out = np.empty_like(x)

    ml = x > 700.0
    ms = x < 1e-3
    mm = ~(ml | ms)

    out[ml] = np.exp(-x[ml])
    out[mm] = 1.0 / np.expm1(x[mm])

    xs = x[ms]
    out[ms] = (1.0 / xs) - 0.5 + (xs / 12.0) - (xs**3) / 720.0
    return out

def solve_lambda_c(T, alpha, beta):
    """Solve for λ_c [cm] in Casey (2012) definition."""
    def g(x):
        if x <= 0:
            return 1e9
        ex = math.exp(x)
        return (3.0 + beta) - (x * ex) / (ex - 1.0) - alpha

    a, b = 1.0e-3, 50.0
    for _ in range(80):
        m = 0.5 * (a + b)
        if g(a) * g(m) <= 0:
            b = m
        else:
            a = m
    x = 0.5 * (a + b)

    nu_c = x * kB * T / h_ergs
    return c_cms / nu_c  # [cm]

def _casey_Snu(nu, T, beta, alpha, nu0, nuc):
    """Casey (2012) S_nu(ν)"""
    nu = np.asarray(nu, np.float64)
    x   = (h_ergs * nu) / (kB * T)
    tau = (nu / nu0)**beta

    S_mbb = (1.0 - np.exp(-tau)) * (2.0 * h_ergs * nu**3) / (c_cms**2) * inv_expm1_stable(x)

    x_c   = (h_ergs * nuc) / (kB * T)
    tau_c = (nuc / nu0)**beta
    S_mbb_c = (
        (1.0 - np.exp(-tau_c)) * (2.0 * h_ergs * nuc**3) /
        (c_cms**2) * inv_expm1_stable(np.array([x_c]))[0]
    )

    Npl = math.e * S_mbb_c * (nuc**alpha)
    S_pl = Npl * (nu**(-alpha)) * np.exp(-(nuc / nu)**2)
    return S_mbb + S_pl

def casey_Slambda_per_cm(lam_cm, T, beta, alpha, lambda0_um, lam_c_cm):
    """Casey S_lambda from S_nu."""
    lam_cm = np.asarray(lam_cm, np.float64)
    nu     = c_cms / lam_cm

    nu0 = c_cms / (float(lambda0_um) * 1e-4)
    nuc = c_cms / lam_c_cm

    S_nu = _casey_Snu(nu, T, beta, alpha, nu0, nuc)
    return S_nu * (c_cms / (lam_cm**2))

# =========================
# 4) Load cubes/templates
# =========================
def load_mist_cubes(cube_dir):
    cubes = {}
    pat = os.path.join(cube_dir, "MIST_z*.hdf5")
    regex = re.compile(r"MIST_z([-\d\.]+)\.hdf5$")

    for fp in glob.glob(pat):
        m = regex.search(os.path.basename(fp))
        if not m:
            continue

        z_val = float(m.group(1))
        with h5py.File(fp, "r") as f:
            cubes[z_val] = {
                "logZ"    : f["logZ"][:].astype(np.float64),
                "logAge" : f["log_age"][:].astype(np.float64),
                "wave"    : f["wavelength"][:].astype(np.float64),
                "sed"     : f["sed"][:].astype(np.float64),   # (n_Z, n_age, n_wave_native)
            }

    if not cubes:
        raise RuntimeError(f"No MIST cube files found in {cube_dir}")
    return dict(sorted(cubes.items(), key=lambda kv: kv[0]))

def load_skirtor_templates(skirtor_dir):
    pat = os.path.join(
        skirtor_dir,
        "t7_p1_q1_oa40_R20_Mcl0.97_i*_sed.dat"
    )

    lib, lam_A = {}, None

    for fn in glob.glob(pat):
        ang = int(os.path.basename(fn).split("_i")[1].split("_")[0])

        lam_um, lamF_Wm2 = np.loadtxt(
            fn, comments="#", usecols=(0, 1), unpack=True
        )
        F_Wm2 = lamF_Wm2 / lam_um
        F_cgs = F_Wm2 * 1e3  # W/m^2 → erg/s/cm^2

        D_ref = 3.085678e25
        L_mu  = F_cgs * 4 * np.pi * D_ref**2
        L_A   = L_mu / 1e4        # per μm -> per Å

        L_ref = Lsun * 1e11
        lib[ang] = (L_A / L_ref).astype(np.float64)

        if lam_A is None:
            lam_A = lam_um * 1e4

    if not lib:
        raise RuntimeError("SKIRTOR templates not found.")
    return np.array(sorted(lib.keys()), dtype=int), lam_A.astype(np.float64), lib

def load_nebular_templates(line_file, cont_file):
    line_raw = np.loadtxt(line_file)
    cont_raw = np.loadtxt(cont_file)
    return line_raw, cont_raw

# =========================
# 5) Nebular helpers
# =========================
def precompute_neb_line_basis(lam_grid, lam_lines, fwhm_kms):
    """Gaussian line basis per line, normalized to ∫ dλ = 1."""
    lam_grid  = np.asarray(lam_grid, np.float64)
    lam_lines = np.asarray(lam_lines, np.float64)

    sigma_factor = fwhm_kms / (c_kms * 2.0 * np.sqrt(2.0 * np.log(2.0)))
    sig = lam_lines * sigma_factor

    basis = np.empty((lam_grid.size, lam_lines.size), dtype=np.float64)
    sqrt2pi = np.sqrt(2.0 * np.pi)

    for j, (ll, s) in enumerate(zip(lam_lines, sig)):
        if s <= 0 or not np.isfinite(s):
            basis[:, j] = 0.0
            continue

        x = (lam_grid - ll) / s
        prof = np.exp(-0.5 * x * x)
        prof /= (s * sqrt2pi)  # ∫prof dλ = 1
        basis[:, j] = prof.astype(np.float64)

    return basis  # sed_lines = basis @ L_lines

def precompute_cont_tail_slope_per_iz(lam_cont, cont_raw, tail_n=12):
    """Per-metallicity tail slope m (ln y vs ln x) for high-λ end."""
    lam = lam_cont.astype(np.float64)
    slopes = []

    for iz in range(cont_raw.shape[1] - 1):  # first col = λ, then each iz
        Lnu_perQ = cont_raw[:, 1 + iz].astype(np.float64)
        Flam1 = Lnu_perQ * (c_cms * 1e8) / (lam**2)  # L_lambda per Q_H=1

        k = int(min(max(tail_n, 2), len(lam)))
        xt = np.log(lam[-k:])
        yt = np.log(np.clip(Flam1[-k:], 1e-300, None))

        m, _ = np.polyfit(xt, yt, 1)
        slopes.append(m)

    return np.array(slopes, dtype=np.float64)  # (n_iz,)

# =========================
# 6) Runtime data preload
# =========================
def preload_runtime_inputs(snap_path, fof_path):
    """
    Load frequently accessed FOF/SNAPSHOT arrays into RAM once.
    This avoids repeated HDF5 random reads during Stage A/B workers.
    """
    print("Preloading FOF/SNAPSHOT arrays into RAM ...")

    with h5py.File(fof_path, "r") as f:
        fof_mem = {
            "sub_mtype4": f["Subhalo/SubhaloMassType"][:, 4].astype(np.float64),
            "zgas_sfr": f["Subhalo/SubhaloMetallicitySFR"][:].astype(np.float64),
            "zgas_instar": f["Subhalo/SubhaloGasMetallicityInStarRad"][:].astype(np.float64),
            "off_star": f["Subhalo/SubhaloOffsetType"][:, 4].astype(np.int64),
            "len_star": f["Subhalo/SubhaloLenType"][:, 4].astype(np.int64),
            "off_bh": f["Subhalo/SubhaloOffsetType"][:, 5].astype(np.int64),
            "len_bh": f["Subhalo/SubhaloLenType"][:, 5].astype(np.int64),
            "spin": f["Subhalo/SubhaloSpin"][:].astype(np.float64),
        }

    with h5py.File(snap_path, "r") as f:
        mdot_ds = f["PartType5/BlackHoleMassAccretionRate"]
        snap_mem = {
            "a_now": float(f["Header"].attrs["Time"]),
            "pt4_mass": f["PartType4/Masses"][:].astype(np.float64),
            "pt4_aform": f["PartType4/StellarFormationTime"][:].astype(np.float64),
            "pt4_Z": f["PartType4/Metallicity"][:].astype(np.float64),
            "pt5_mdot": mdot_ds[:].astype(np.float64),
            "pt5_mdot_to_cgs": float(mdot_ds.attrs.get("to_cgs", 1.0)),
        }

    return snap_mem, fof_mem

# =========================
# Stage A: Stellar (ADAPT), Q_H
# =========================
def stage_stellar(eligible_sids, cubes, z_keys, a_now,
                  out_path, fof_mem, snap_mem, rows_chunk=None):
    register_tmp(out_path)

    N_sel  = len(eligible_sids)
    N_wave = ADAPT_WAVE.size

    if rows_chunk is None:
        rows_chunk = choose_rows_chunk(
            TARGET_RAM_GB * 0.15,
            N_wave,
            arrays_in_block=2,   # stellar + maybe scratch
            itemsize=8,
            safety_gb=2.0,
            hard_cap=20000,
        )
    rows_chunk = cap_rows_chunk_for_hdf5(N_wave, rows_chunk, dtype=np.float64)
    rows_chunk = min(rows_chunk, N_sel)
    task_rows = max(1, min(rows_chunk, BLOCK_ROWS_STAGE_A))

    z_keys = np.asarray(z_keys, dtype=np.float64)
    if z_keys.ndim != 1 or z_keys.size == 0:
        raise ValueError("z_keys must be a non-empty 1D array")
    if np.any(np.diff(z_keys) < 0):
        raise ValueError("z_keys must be sorted in ascending order")

    # O(log N) nearest-redshift binning for sorted z_keys.
    z_mid = 0.5 * (z_keys[:-1] + z_keys[1:]) if z_keys.size > 1 else np.empty((0,), dtype=np.float64)
    cube_by_k = [cubes[float(z)] for z in z_keys]
    interp_map_by_k = [precompute_interp_map_loglog(cb["wave"], ADAPT_WAVE) for cb in cube_by_k]
    t_now = float(cosmic_time_from_a(a_now))

    off_star = fof_mem["off_star"]
    len_star = fof_mem["len_star"]
    pt4_mass = snap_mem["pt4_mass"]
    pt4_aform = snap_mem["pt4_aform"]
    pt4_Z = snap_mem["pt4_Z"]

    with h5py.File(out_path, "w",
                   libver="latest",
                   rdcc_nbytes=(1 << 28),
                   rdcc_nslots=(1 << 20)) as f_out:

        f_out.create_dataset("wavelength_adapt", data=ADAPT_WAVE, dtype="f8")
        f_out.create_dataset("subhalo_ids", data=eligible_sids.astype(np.int32), dtype="i4")

        d_stellar = f_out.create_dataset(
            "stellar",
            shape=(N_sel, N_wave),
            dtype="f8",
            chunks=(rows_chunk, N_wave),
        )
        d_QH = f_out.create_dataset("Q_H", shape=(N_sel,), dtype="f8")

        def work_block(start, end):
            n_block = end - start
            stellar_block = np.zeros((n_block, N_wave), dtype=np.float64)
            qh_block = np.zeros((n_block,), dtype=np.float64)

            for bi, sid in enumerate(eligible_sids[start:end]):
                sid = int(sid)
                b = int(off_star[sid])
                e = int(off_star[sid] + len_star[sid])

                spec_adapt_sum = np.zeros(N_wave, dtype=np.float64)

                if e > b:
                    m_star_sim = pt4_mass[b:e]
                    a_form = pt4_aform[b:e]
                    Z_abs_star = pt4_Z[b:e]

                    m_physical = m_star_sim * MASS_CONV
                    t_form = cosmic_time_from_a(a_form)
                    ages_s = np.maximum((t_now - t_form) * 1e9, 1e-5)
                    logA = np.log10(ages_s)
                    logZZ = np.log10(np.clip(Z_abs_star / Z_sun, 1e-6, None))

                    z_form = (1.0 / np.asarray(a_form)) - 1.0
                    if z_mid.size:
                        z_pick_idx = np.searchsorted(z_mid, z_form, side="left").astype(np.int32)
                    else:
                        z_pick_idx = np.zeros(z_form.shape[0], dtype=np.int32)

                    for k in np.unique(z_pick_idx):
                        sel = (z_pick_idx == k)
                        cube = cube_by_k[int(k)]

                        iZ = np.searchsorted(cube["logZ"], logZZ[sel]).clip(
                            0, cube["logZ"].size - 1
                        )
                        iA = np.searchsorted(cube["logAge"], logA[sel]).clip(
                            0, cube["logAge"].size - 1
                        )
                        w = m_physical[sel]

                        nA = cube["logAge"].size
                        key = (iZ.astype(np.int64) * nA) + iA.astype(np.int64)
                        uniq, inv = np.unique(key, return_inverse=True)

                        wsum = np.zeros(uniq.size, dtype=np.float64)
                        np.add.at(wsum, inv, w)

                        uniq_iZ = (uniq // nA).astype(np.int32)
                        uniq_iA = (uniq % nA).astype(np.int32)

                        seds_nat = cube["sed"][uniq_iZ, uniq_iA].astype(np.float64, copy=False)
                        idxlo, idxhi, wlo, whi, left_mask, right_mask, _ = interp_map_by_k[int(k)]

                        spec_native = np.einsum("ij,i->j", seds_nat, wsum, optimize=True)
                        spec_adapt = (spec_native[idxlo] * wlo) + (spec_native[idxhi] * whi)
                        spec_adapt[left_mask | right_mask] = 0.0
                        spec_adapt_sum += spec_adapt

                stellar_block[bi, :] = spec_adapt_sum
                qh_block[bi] = compute_QH(ADAPT_WAVE, spec_adapt_sum)

            return start, end, stellar_block, qh_block

        def consume_block(result):
            start, end, stellar_block, qh_block = result
            d_stellar[start:end, :] = stellar_block
            d_QH[start:end] = qh_block

        run_parallel_row_blocks(
            n_rows=N_sel,
            block_rows=task_rows,
            desc="Stage A: Stellar",
            unit="sid",
            worker_fn=work_block,
            consume_fn=consume_block,
        )

# =========================
# Stage B: AGN (ADAPT)
# =========================
def stage_agn(eligible_sids, angles, lam_agn, skirtor_lib,
              out_path, fof_mem, snap_mem, rows_chunk=None):
    register_tmp(out_path)

    N_sel  = len(eligible_sids)
    N_wave = ADAPT_WAVE.size

    if rows_chunk is None:
        rows_chunk = choose_rows_chunk(
            TARGET_RAM_GB * 0.05,
            N_wave,
            arrays_in_block=1,
            itemsize=8,
            safety_gb=1.0,
            hard_cap=20000,
        )
    rows_chunk = cap_rows_chunk_for_hdf5(N_wave, rows_chunk, dtype=np.float64)
    rows_chunk = min(rows_chunk, N_sel)
    task_rows = max(1, min(rows_chunk, BLOCK_ROWS_STAGE_B))

    off_bh = fof_mem["off_bh"]
    len_bh = fof_mem["len_bh"]
    spin = fof_mem["spin"]
    pt5_mdot = snap_mem["pt5_mdot"]
    to_cgs = float(snap_mem["pt5_mdot_to_cgs"])

    with h5py.File(out_path, "w",
                   libver="latest",
                   rdcc_nbytes=(1 << 27),
                   rdcc_nslots=(1 << 20)) as f_out:
        f_out.create_dataset("wavelength_adapt", data=ADAPT_WAVE, dtype="f8")
        f_out.create_dataset("subhalo_ids", data=eligible_sids.astype(np.int32), dtype="i4")

        d_agn = f_out.create_dataset(
            "agn",
            shape=(N_sel, N_wave),
            dtype="f8",
            chunks=(rows_chunk, N_wave),
        )

        def work_block(start, end):
            n_block = end - start
            agn_block = np.zeros((n_block, N_wave), dtype=np.float64)

            for bi, sid in enumerate(eligible_sids[start:end]):
                sid = int(sid)
                jb = int(off_bh[sid])
                je = int(off_bh[sid] + len_bh[sid])
                if je <= jb:
                    continue

                mdot = pt5_mdot[jb:je] * to_cgs
                Lbol = 0.10 * mdot * c_cms**2
                Ltot = float(np.sum(Lbol))
                if Ltot <= 0:
                    continue

                j = spin[sid]
                jn = np.linalg.norm(j)
                if jn > 0:
                    inc = np.degrees(np.arccos(np.clip(j[2] / jn, -1, 1)))
                else:
                    inc = 0.0

                ang = angles[np.argmin(np.abs(angles - inc))]
                sedA = skirtor_lib[ang] * Ltot
                agn_block[bi, :] = interp_loglog_to_grid(lam_agn, sedA, ADAPT_WAVE)

            return start, end, agn_block

        def consume_block(result):
            start, end, agn_block = result
            d_agn[start:end, :] = agn_block

        run_parallel_row_blocks(
            n_rows=N_sel,
            block_rows=task_rows,
            desc="Stage B: AGN",
            unit="sid",
            worker_fn=work_block,
            consume_fn=consume_block,
        )

# =========================
# Stage C: Nebular (ADAPT) using Z_gas and Te(Z_gas)
# =========================
def stage_nebular(eligible_sids, line_raw, cont_raw,
                  stellar_tmp, out_path, Z_gas_sel, rows_chunk=None):
    register_tmp(out_path)
    assert ADAPT_WAVE is not None

    N_sel  = len(eligible_sids)
    N_wave = ADAPT_WAVE.size

    if rows_chunk is None:
        rows_chunk = choose_rows_chunk(
            TARGET_RAM_GB * 0.10,
            N_wave,
            arrays_in_block=1,
            itemsize=8,
            safety_gb=2.0,
            hard_cap=20000,
        )
    rows_chunk = cap_rows_chunk_for_hdf5(N_wave, rows_chunk, dtype=np.float64)
    rows_chunk = min(rows_chunk, N_sel)
    task_rows = max(1, min(rows_chunk, BLOCK_ROWS_STAGE_C))

    with h5py.File(stellar_tmp, "r") as fs:
        QH_array = fs["Q_H"][:].astype(np.float64)

    Z_gas_sel = np.asarray(Z_gas_sel, dtype=np.float64)
    if Z_gas_sel.shape[0] != N_sel:
        raise ValueError("Z_gas_sel must have length == number of eligible subhaloes")

    lam_line = line_raw[:, 1].astype(np.float64)
    neb_basis = precompute_neb_line_basis(ADAPT_WAVE, lam_line, FWHM_KMS)

    lam_cont = cont_raw[:, 0].astype(np.float64)
    idxlo_c, idxhi_c, wlo_c, whi_c, left_mask_c, right_mask_c, _ = \
        precompute_interp_map_loglog(lam_cont, ADAPT_WAVE)
    has_right_tail = bool(np.any(right_mask_c))
    cont_to_lam = (c_cms * 1e8) / (lam_cont**2)

    tail_slopes = precompute_cont_tail_slope_per_iz(lam_cont, cont_raw, tail_n=TAIL_N)
    tail_slopes = np.asarray(tail_slopes, dtype=np.float64)
    xmax = lam_cont[-1]
    log_dst = np.log(ADAPT_WAVE.astype(np.float64))

    nZ = int(Z_GRID_FOR_TPL.size)
    if cont_raw.shape[1] < (1 + nZ):
        raise ValueError("Nebular continuum template columns do not match Z_GRID_FOR_TPL length.")
    max_ratio_col = 2 + 2 * (nZ - 1)
    if line_raw.shape[1] <= max_ratio_col:
        raise ValueError("Nebular line template columns do not match Z_GRID_FOR_TPL length.")
    if tail_slopes.size < nZ:
        raise ValueError("Tail-slope vector is shorter than the nebular metallicity grid.")

    line_ratio_by_z = np.stack(
        [line_raw[:, 2 + 2 * iz].astype(np.float64) for iz in range(nZ)],
        axis=1,
    )  # (N_lines, nZ)
    cont_lnu_perQ_by_z = np.stack(
        [cont_raw[:, 1 + iz].astype(np.float64) for iz in range(nZ)],
        axis=1,
    )  # (N_cont, nZ)
    tail_slopes = tail_slopes[:nZ]

    pos_mask = (Z_GRID_FOR_TPL >= Z_FLOOR_NEB)
    pos_idx = np.nonzero(pos_mask)[0].astype(np.int32)
    if pos_idx.size < 2:
        raise ValueError("Need at least two positive-Z nodes (>= Z_FLOOR_NEB) for logZ interpolation.")
    logZ_pos = np.log10(Z_GRID_FOR_TPL[pos_idx].astype(np.float64))
    order = np.argsort(logZ_pos)
    pos_idx = pos_idx[order]
    logZ_pos = logZ_pos[order]
    if np.any(np.diff(logZ_pos) <= 0.0):
        raise ValueError("Positive-Z metallicity nodes must be strictly increasing in logZ.")

    def sed_cont_with_tail(Flam_native, m_tail):
        inside = (Flam_native[idxlo_c] * wlo_c) + (Flam_native[idxhi_c] * whi_c)
        out = inside.copy()

        if has_right_tail:
            m = float(m_tail)
            y_at_xmax = float(Flam_native[-1])
            b = np.log(max(y_at_xmax, 1e-300)) - m * np.log(xmax)
            out[right_mask_c] = np.exp(m * log_dst[right_mask_c] + b)

        out[left_mask_c] = 0.0
        out[~np.isfinite(out)] = 0.0
        out[out < 0] = 0.0
        return out

    def metallicity_interp_weights_logZ(Zgas):
        """
        Return (iz_lo, iz_hi, w_lo, w_hi, Z_out) for nebular-template interpolation.
        For Z < 1e-7 (or non-finite / non-positive), use the dedicated Z=0 bin directly.
        """
        z = float(Zgas)
        if (not np.isfinite(z)) or (z <= 0.0) or (z < Z_FLOOR_NEB):
            return IDX_Z_ZERO_TPL, IDX_Z_ZERO_TPL, 1.0, 0.0, 0.0

        z = min(max(z, float(Z_FLOOR_NEB)), float(Z_GRID_FOR_TPL.max()))
        logz = math.log10(z)

        hi = int(np.searchsorted(logZ_pos, logz, side="left"))
        hi = min(max(hi, 1), logZ_pos.size - 1)
        lo = hi - 1

        x0 = float(logZ_pos[lo])
        x1 = float(logZ_pos[hi])
        if (not np.isfinite(x0)) or (not np.isfinite(x1)) or (x1 <= x0):
            iz = int(pos_idx[lo])
            return iz, iz, 1.0, 0.0, z

        w_hi = float(np.clip((logz - x0) / (x1 - x0), 0.0, 1.0))
        w_lo = 1.0 - w_hi
        return int(pos_idx[lo]), int(pos_idx[hi]), w_lo, w_hi, z

    def work_block(start, end):
        n_block = end - start
        neb_block = np.zeros((n_block, N_wave), dtype=np.float64)

        for bi, row_idx in enumerate(range(start, end)):
            QH = float(QH_array[row_idx])
            if QH <= 0.0:
                continue

            Zgas_in = float(Z_gas_sel[row_idx])
            iz_lo, iz_hi, w_lo, w_hi, Zgas = metallicity_interp_weights_logZ(Zgas_in)

            # Te uses the piecewise Te_from_Zgas rule (fixed floor below Z<1e-7).
            Te = Te_from_Zgas(Zgas)

            # Line emission: QH -> L(Hbeta) uses Te
            L_Hb = (gamma_Hbeta(Te) / alpha_B(Te)) * QH

            # Interpolate line ratios in logZ (template values linearly combined).
            ratios = (
                line_ratio_by_z[:, iz_lo] * w_lo
                + line_ratio_by_z[:, iz_hi] * w_hi
            )
            ratios[~np.isfinite(ratios)] = 0.0
            ratios = np.clip(ratios, 0.0, None)
            L_lines = ratios * L_Hb
            sed_lines = (neb_basis @ L_lines)  # (N_wave,)

            # Interpolate continuum in logZ (template values linearly combined), then scale by QH.
            Lnu_perQ = (
                cont_lnu_perQ_by_z[:, iz_lo] * w_lo
                + cont_lnu_perQ_by_z[:, iz_hi] * w_hi
            )
            Lnu_perQ[~np.isfinite(Lnu_perQ)] = 0.0
            Lnu_perQ = np.clip(Lnu_perQ, 0.0, None)
            Flam_native = (Lnu_perQ * QH) * cont_to_lam

            m_tail = (
                float(tail_slopes[iz_lo]) * w_lo
                + float(tail_slopes[iz_hi]) * w_hi
            )
            sed_cont = sed_cont_with_tail(Flam_native, m_tail)

            neb_block[bi, :] = (sed_lines + sed_cont).astype(np.float64)

        return start, end, neb_block

    with h5py.File(out_path, "w",
                   libver="latest",
                   rdcc_nbytes=(1 << 27),
                   rdcc_nslots=(1 << 20)) as f_out:
        f_out.create_dataset("wavelength_adapt", data=ADAPT_WAVE, dtype="f8")
        f_out.create_dataset("subhalo_ids", data=eligible_sids.astype(np.int32), dtype="i4")

        d_neb = f_out.create_dataset(
            "nebular",
            shape=(N_sel, N_wave),
            dtype="f8",
            chunks=(rows_chunk, N_wave),
        )

        def consume_block(result):
            start, end, neb_block = result
            d_neb[start:end, :] = neb_block

        run_parallel_row_blocks(
            n_rows=N_sel,
            block_rows=task_rows,
            desc="Stage C: Nebular",
            unit="sid",
            worker_fn=work_block,
            consume_fn=consume_block,
        )

def stage_merge_and_cleanup(eligible_sids, stellar_tmp, agn_tmp, neb_tmp,
                            out_final, a_now, E_B_V_snapshot, rows_chunk=None):
    assert ADAPT_WAVE is not None

    N_sel  = len(eligible_sids)
    N_wave = ADAPT_WAVE.size

    if rows_chunk is None:
        rows_chunk = choose_rows_chunk(
            TARGET_RAM_GB * 0.65,
            N_wave,
            arrays_in_block=6,
            itemsize=8,
            safety_gb=10.0,
            hard_cap=15000,
        )
    rows_chunk = cap_rows_chunk_for_hdf5(N_wave, rows_chunk, dtype=np.float64)
    rows_chunk = min(rows_chunk, N_sel)

    # Snapshot redshift & E(B-V)
    z_snapshot = (1.0 / a_now) - 1.0
    E_B_V = float(E_B_V_snapshot)

    # Precompute attenuation transmission and masks
    lam_um = ADAPT_WAVE.astype(np.float64) * 1e-4
    k_lam  = k_calzetti_lambda_um(lam_um, Rv_prime=RV_PRIME)
    trans  = np.power(10.0, -0.4 * (k_lam * E_B_V)).astype(np.float64)
    mask_ion = (ADAPT_WAVE <= 912.0)

    # Casey shape (row-invariant)
    lam_norm_um = np.geomspace(IR_MIN_UM, IR_MAX_UM, 2000)
    lam_norm_cm = lam_norm_um * 1e-4
    lam_c_cm    = solve_lambda_c(T_dust, CASEY_ALPHA, CASEY_BETA)
    S_shape_norm = casey_Slambda_per_cm(
        lam_norm_cm, T_dust, CASEY_BETA, CASEY_ALPHA,
        CASEY_LAMBDA0_UM, lam_c_cm,
    )
    area = integrate_trapz(lam_norm_cm, S_shape_norm)

    casey_shape_perA = np.zeros(N_wave, dtype=np.float64)
    mask_ir = (lam_um >= IR_MIN_UM) & (lam_um <= IR_MAX_UM)
    if np.any(mask_ir):
        lam_cm = (ADAPT_WAVE[mask_ir].astype(np.float64) * 1e-8)
        S_per_cm = casey_Slambda_per_cm(
            lam_cm, T_dust, CASEY_BETA, CASEY_ALPHA,
            CASEY_LAMBDA0_UM, lam_c_cm,
        )
        casey_shape_perA[mask_ir] = (S_per_cm * 1e-8)  # per cm → per Å

    mask_abs = (lam_um >= L_ABS_MIN_UM) & (lam_um <= L_ABS_MAX_UM)

    # Open temp stage files
    fs = h5py.File(stellar_tmp, "r",
                   rdcc_nbytes=(1 << 28),
                   rdcc_nslots=(1 << 20))
    fa = h5py.File(agn_tmp, "r",
                   rdcc_nbytes=(1 << 28),
                   rdcc_nslots=(1 << 20))
    fn = h5py.File(neb_tmp, "r",
                   rdcc_nbytes=(1 << 28),
                   rdcc_nslots=(1 << 20))

    stellar_ad = fs["stellar"]
    agn_ad     = fa["agn"]
    neb_ad     = fn["nebular"]

    with h5py.File(out_final, "w",
                   libver="latest",
                   rdcc_nbytes=(1 << 29),
                   rdcc_nslots=(1 << 20)) as f_out:
        f_out.create_dataset("wavelength", data=ADAPT_WAVE, dtype="f8")
        f_out.create_dataset("subhalo_ids", data=eligible_sids.astype(np.int32), dtype="i4")

        filters_final = dict(
            compression="gzip",
            compression_opts=1,
            shuffle=True,
        )

        rows_chunk = cap_rows_chunk_for_hdf5(N_wave, rows_chunk, dtype=np.float64)

        # Create only requested datasets
        d_out = {}
        def maybe_create(name):
            if SAVE(name):
                d_out[name] = f_out.create_dataset(
                    name,
                    shape=(N_sel, N_wave),
                    dtype="f8",
                    chunks=(rows_chunk, N_wave),
                    **filters_final,
                )
            else:
                d_out[name] = None

        maybe_create("stellar_sed")
        maybe_create("nebular_sed")
        maybe_create("stellar_plus_nebular")
        maybe_create("stellar_nebular_agn_attenuated")
        maybe_create("dust_emission_sed")
        maybe_create("agn_sed")
        maybe_create("total_sed")

        pbar = tqdm(total=N_sel, desc="Stage D: Merge", unit="sid")

        for start in range(0, N_sel, rows_chunk):
            end = min(N_sel, start + rows_chunk)

            st_block = stellar_ad[start:end, :].astype(np.float64, copy=False)
            ag_block = agn_ad[start:end, :].astype(np.float64, copy=False)
            nb_block = neb_ad[start:end, :].astype(np.float64, copy=False)

            if d_out["stellar_sed"] is not None:
                d_out["stellar_sed"][start:end, :] = st_block
            if d_out["nebular_sed"] is not None:
                d_out["nebular_sed"][start:end, :] = nb_block
            if d_out["agn_sed"] is not None:
                d_out["agn_sed"][start:end, :] = ag_block

            need_spn = any(
                d_out[k] is not None
                for k in ("stellar_plus_nebular", "stellar_nebular_agn_attenuated", "dust_emission_sed", "total_sed")
            )
            if need_spn:
                spn = st_block + nb_block
                if d_out["stellar_plus_nebular"] is not None:
                    d_out["stellar_plus_nebular"][start:end, :] = spn
            else:
                spn = None

            need_atten = any(
                d_out[k] is not None
                for k in ("stellar_nebular_agn_attenuated", "dust_emission_sed", "total_sed")
            )
            if need_atten:
                spnag = ((spn if spn is not None else (st_block + nb_block)) + ag_block)
                atten_all = spnag * trans
                atten_all[:, mask_ion] = 0.0

                if d_out["stellar_nebular_agn_attenuated"] is not None:
                    d_out["stellar_nebular_agn_attenuated"][start:end, :] = atten_all
            else:
                spnag = None
                atten_all = None

            need_dust = any(
                d_out[k] is not None
                for k in ("dust_emission_sed", "total_sed")
            )
            if need_dust:
                if np.any(mask_abs):
                    lam_abs = ADAPT_WAVE[mask_abs].astype(np.float64)
                    base_for_abs = spnag if spnag is not None else (st_block + nb_block + ag_block)

                    diff_abs = np.clip(base_for_abs[:, mask_abs] - atten_all[:, mask_abs], 0.0, None)
                    L_abs = np.trapz(diff_abs, lam_abs, axis=1)
                else:
                    L_abs = np.zeros((end - start,), dtype=np.float64)

                if area > 0:
                    dust = (L_abs[:, None] / area) * casey_shape_perA[None, :]
                    dust[~np.isfinite(dust)] = 0.0
                else:
                    dust = np.zeros_like(atten_all, dtype=np.float64)

                if d_out["dust_emission_sed"] is not None:
                    d_out["dust_emission_sed"][start:end, :] = dust
            else:
                dust = None

            if d_out["total_sed"] is not None:
                total = (
                    atten_all if atten_all is not None else np.zeros_like(st_block)
                ) + (dust if dust is not None else 0.0)
                d_out["total_sed"][start:end, :] = total

            pbar.update(end - start)

        pbar.close()

        # Basic metadata
        f_out.attrs["units"] = (
            "All SEDs are L_lambda [erg s^-1 Angstrom^-1], "
            "wavelength in Angstrom (rest)."
        )
        f_out.attrs["dtype"] = "float64"
        f_out.attrs["Rv_prime"] = float(RV_PRIME)
        f_out.attrs["z_snapshot"] = float(z_snapshot)
        f_out.attrs["E(B-V)"] = float(E_B_V)
        f_out.attrs["E(B-V)_Hayes_C"] = float(C_EBV_HAYES)
        f_out.attrs["E(B-V)_Hayes_z_e_fold"] = float(Z_EBV_HAYES)
        f_out.attrs["Te_table_note"] = (
            "Te from metallicity: fixed Z=0 value for Z<1e-7, "
            "logZ-logTe interpolation for Z>=1e-7."
        )

    fs.close()
    fa.close()
    fn.close()

    for fp in [TMP_STELLAR, TMP_AGN, TMP_NEB]:
        try:
            if os.path.exists(fp):
                os.remove(fp)
                TO_DELETE.discard(fp)
        except Exception:
            pass

# =========================
# Main
# =========================
def main():
    print("Loading Stellar templates ...")
    cubes = load_mist_cubes(Stellar_DIR)
    z_keys = np.array(list(cubes.keys()), dtype=float)

    print("Loading templates (nebular, AGN) ...")
    line_raw, cont_raw = load_nebular_templates(NEB_LINE_FILE, NEB_CONT_FILE)
    angles, lam_agn, skirtor_lib = load_skirtor_templates(AGN_DIR)

    # Preload frequently-used snapshot/fof arrays in RAM once
    snap_mem, fof_mem = preload_runtime_inputs(SNAP, FOF)

    # SNAP scalar (scale factor)
    a_now = float(snap_mem["a_now"])

    z_now = (1.0 / a_now) - 1.0
    E_B_V_now = ebv_from_redshift(z_now)
    print(f"Snapshot: a = {a_now:.6f}, z = {z_now:.3f}, E(B-V) = {E_B_V_now:.3f} (Hayes+2011)")

    # Selection + gas metallicity arrays (FOF)
    print("Selecting subhaloes by M★ > 1e9 Msun ...")
    sub_mtype = fof_mem["sub_mtype4"]
    Mstar = sub_mtype * MASS_CONV
    eligible_sids = np.nonzero(Mstar > MSTAR_THRESH)[0].astype(np.int64)

    # Gas metallicity to use for nebular:
    Zgas_sfr_all = fof_mem["zgas_sfr"]
    Zgas_instar_all = fof_mem["zgas_instar"]

    N_sel = int(eligible_sids.size)
    print(f"Eligible subhaloes: {N_sel}")
    if N_sel == 0:
        print("No subhalo passes the mass threshold. Exiting.")
        return

    Zgas_sfr_sel = Zgas_sfr_all[eligible_sids]
    Zgas_instar_sel = Zgas_instar_all[eligible_sids]
    valid = np.isfinite(Zgas_sfr_sel) & (Zgas_sfr_sel > 0.0)
    Z_gas_sel = np.where(valid, Zgas_sfr_sel, Zgas_instar_sel)
    # final safety clamp (avoid <=0 or NaN)
    bad = (~np.isfinite(Z_gas_sel)) | (Z_gas_sel <= 0.0)
    if np.any(bad):
        Z_gas_sel[bad] = float(Z_GRID_FOR_TPL[-1])

    # Adaptive wavelength grid (target ~30k + line windows)
    lam_lines = line_raw[:, 1].astype(np.float64)
    global ADAPT_WAVE
    ADAPT_WAVE = build_adaptive_wave(
        base_min_A=91.0,
        base_max_A=1.0e7,
        n_base=30000,
        lam_lines=lam_lines,
        fwhm_kms=FWHM_KMS,
        W_sigma=LINE_W_SIGMA,
        p_per_fwhm=LINE_P_PER_FWHM,
        edge_points=True,
    )
    print(f"Adaptive wavelength grid built: N={ADAPT_WAVE.size} (target~30k)")

    # Planner log (Stage D chunk size)
    rows_chunk_D = choose_rows_chunk(
        TARGET_RAM_GB * 0.65,
        ADAPT_WAVE.size,
        arrays_in_block=6,
        itemsize=8,
        safety_gb=10.0,
        hard_cap=15000,
    )
    rows_chunk_D_capped = cap_rows_chunk_for_hdf5(ADAPT_WAVE.size, rows_chunk_D, dtype=np.float64)
    print(f"[Planner] Stage D rows_chunk (raw≈{rows_chunk_D}) → capped={rows_chunk_D_capped} (HDF5 <4GB/chunk, f64)")

    # Stages
    stage_stellar(eligible_sids, cubes, z_keys, a_now, TMP_STELLAR, fof_mem, snap_mem)
    stage_agn(eligible_sids, angles, lam_agn, skirtor_lib, TMP_AGN, fof_mem, snap_mem)
    stage_nebular(eligible_sids, line_raw, cont_raw, TMP_STELLAR, TMP_NEB, Z_gas_sel)

    # Stage D (merge + cleanup) with E(B-V)(z)
    stage_merge_and_cleanup(
        eligible_sids, TMP_STELLAR, TMP_AGN, TMP_NEB,
        OUT_FINAL, a_now, E_B_V_now
    )

    print(f"\n✓ All done. Final file → {os.path.abspath(OUT_FINAL)}")

if __name__ == "__main__":
    main()
